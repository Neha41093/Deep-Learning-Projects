{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_Speaker_Verification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "h6hmAUPcRxeZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Speaker Verification**"
      ]
    },
    {
      "metadata": {
        "id": "_eTJxve-y_HU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "82cde9ea-f851-407b-9698-c95b7e69e596"
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa # in colab, you'll need to install this\n",
        "import librosa\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed\n",
        "import ipywidgets as widgets\n",
        "from math import ceil\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.2.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.3)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.12.5)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.28.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "suphb304R3DI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-NNVG4qR9px",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "11f9edc8-489f-4c0d-981c-8849551b161a"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aa7e1ffc-ed59-48e9-8c02-e7a1642cb2f6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-aa7e1ffc-ed59-48e9-8c02-e7a1642cb2f6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving hw4_tes.pkl to hw4_tes.pkl\n",
            "Saving hw4_trs.pkl to hw4_trs.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X0QQ0kVLSG-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Data Loading and manipulation\n",
        "\n",
        "with open('hw4_trs.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "with open('hw4_tes.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZSh3ffUTwJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cecfd3d-d3f0-45f7-e73f-ff381278841b"
      },
      "cell_type": "code",
      "source": [
        "train_data[0].shape\n",
        "test_data[0].shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16180,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22631,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "_CDjepARTWtJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Getting magnitudes, transpose, padding and forming pairs\n",
        "max_len = 45\n",
        "\n",
        "def speech_file_loading(data, numfiles):\n",
        "    stft_signals = []\n",
        "    abs_signals = []\n",
        "    lengths = []\n",
        "    \n",
        "    for i in range(numfiles):\n",
        "        \n",
        "        #Calculating STFT\n",
        "        stft = librosa.stft(data[i], n_fft= 1024, hop_length= 512)\n",
        "        stftlen = stft.shape[1]\n",
        "        stft_signals.append(stft)\n",
        "        \n",
        "        #Calculating Magnitude values\n",
        "        stftabs = np.abs(stft)\n",
        "        stftabs = np.pad(stftabs, ((0,0),(0, max_len-stftlen)), 'constant')\n",
        "        abs_signals.append(stftabs)\n",
        "        \n",
        "        lengths.append(stftlen)\n",
        "        \n",
        "    return stft_signals, abs_signals, lengths\n",
        "  \n",
        "#Training data\n",
        "X_inp, X_abs, X_len = speech_file_loading(train_data, 500)\n",
        "\n",
        "#Test data\n",
        "X_test_inp, X_test_abs, X_test_len = speech_file_loading(test_data, 200)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lC_oV_0eRe_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c2315af1-4b5b-4785-9da6-aed818c52f52"
      },
      "cell_type": "code",
      "source": [
        "len(X_inp), len(X_abs)\n",
        "len(X_test_inp), len(X_test_abs)\n",
        "X_inp[0].shape\n",
        "X_abs[0].shape\n",
        "X_test_inp[0].shape\n",
        "X_test_abs[0].shape\n",
        "print(X_len)\n",
        "print(X_test_len)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "text": [
            "[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\n",
            "[45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TNYLRm1pUEh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "2de2ce60-93cd-4091-a2f9-5772379a02f4"
      },
      "cell_type": "code",
      "source": [
        "#Getting required batches for the speaker verification task\n",
        "\n",
        "#Training dataset\n",
        "#Get positive batches for each speaker with L=10 i.e. 10 positive pairs --> 20 samples\n",
        "#Get negative batches for each speaker with L=10 i.e. 10 negative pairs --> 20 samples\n",
        "\n",
        "positive_indices = []\n",
        "negative_indices = []\n",
        "for i in range(0,500,10):\n",
        "  positive_indices.append(list(np.random.choice(np.arange(i,i+9), 20, replace=True)))\n",
        "  pos = np.arange(i,i+9)\n",
        "  neg = [j for j in range(500) if j not in pos] \n",
        "  negative_indices.append(list(np.random.choice(pos, 10, replace=True)) + list(np.random.choice(neg, 10, replace=True)))\n",
        "\n",
        "print (positive_indices)\n",
        "print (\"\\n\")\n",
        "print (len(positive_indices))\n",
        "print (\"\\n\")\n",
        "print (negative_indices)\n",
        "print (\"\\n\")\n",
        "print (len(negative_indices))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 4, 8, 8, 7, 8, 2, 7, 2, 5, 3, 1, 2, 6, 7, 1, 7, 2, 5, 5], [16, 15, 14, 11, 11, 12, 16, 10, 14, 15, 13, 13, 10, 15, 17, 14, 16, 10, 18, 18], [20, 28, 21, 27, 25, 25, 22, 26, 27, 28, 21, 20, 26, 21, 25, 20, 26, 22, 21, 27], [34, 30, 32, 31, 34, 31, 38, 30, 37, 36, 33, 32, 30, 31, 36, 35, 32, 36, 32, 36], [40, 48, 42, 43, 43, 44, 42, 48, 43, 42, 45, 40, 46, 46, 40, 43, 45, 42, 48, 45], [53, 51, 50, 55, 53, 52, 51, 51, 50, 54, 50, 55, 57, 58, 58, 55, 51, 58, 55, 55], [67, 67, 68, 64, 63, 67, 64, 68, 63, 60, 64, 68, 66, 68, 68, 62, 65, 66, 60, 65], [77, 73, 75, 78, 76, 73, 70, 72, 78, 71, 78, 76, 74, 71, 72, 71, 75, 73, 78, 72], [82, 83, 84, 85, 87, 84, 88, 86, 84, 80, 82, 81, 88, 86, 87, 86, 82, 83, 84, 88], [95, 98, 93, 96, 92, 91, 97, 94, 90, 94, 95, 93, 95, 91, 92, 90, 96, 95, 91, 92], [101, 105, 100, 100, 101, 107, 106, 102, 105, 100, 104, 104, 106, 107, 105, 107, 105, 104, 107, 104], [113, 113, 112, 115, 118, 111, 114, 117, 117, 116, 113, 117, 114, 118, 113, 113, 110, 111, 113, 118], [122, 121, 123, 126, 126, 124, 123, 123, 121, 120, 122, 128, 127, 123, 126, 127, 123, 122, 126, 122], [135, 133, 138, 131, 138, 135, 130, 137, 131, 136, 133, 135, 132, 132, 134, 138, 133, 130, 136, 133], [146, 147, 145, 140, 143, 140, 140, 144, 143, 143, 142, 143, 144, 146, 144, 144, 140, 148, 145, 142], [152, 153, 155, 156, 153, 157, 157, 153, 156, 151, 153, 152, 151, 153, 151, 154, 154, 157, 156, 156], [166, 160, 160, 166, 162, 162, 166, 161, 165, 162, 165, 162, 168, 166, 166, 160, 166, 165, 160, 164], [174, 171, 171, 178, 177, 177, 175, 176, 178, 170, 173, 172, 175, 175, 176, 177, 170, 178, 172, 175], [188, 183, 185, 182, 188, 185, 180, 188, 187, 185, 186, 184, 185, 181, 182, 182, 183, 185, 188, 185], [197, 193, 191, 194, 191, 197, 195, 196, 195, 196, 192, 193, 193, 190, 197, 194, 194, 191, 194, 194], [200, 201, 206, 206, 207, 200, 207, 204, 207, 202, 201, 204, 207, 203, 206, 205, 200, 207, 203, 200], [211, 215, 218, 212, 215, 217, 215, 217, 218, 217, 218, 215, 215, 212, 215, 218, 218, 217, 211, 211], [226, 228, 223, 227, 221, 225, 227, 224, 223, 224, 224, 223, 224, 227, 224, 228, 220, 221, 224, 228], [230, 231, 235, 230, 236, 237, 233, 233, 231, 237, 235, 231, 232, 234, 235, 230, 233, 231, 233, 234], [241, 244, 248, 244, 242, 247, 247, 243, 242, 248, 242, 240, 248, 244, 240, 246, 247, 242, 241, 245], [253, 250, 258, 257, 255, 256, 252, 256, 253, 256, 257, 258, 253, 252, 253, 257, 255, 250, 252, 253], [263, 267, 268, 261, 263, 266, 260, 265, 261, 260, 265, 262, 263, 260, 268, 268, 262, 266, 264, 263], [277, 274, 278, 274, 278, 273, 273, 271, 272, 275, 273, 276, 276, 276, 272, 277, 273, 272, 272, 274], [280, 285, 284, 283, 284, 286, 284, 281, 287, 284, 288, 281, 288, 287, 282, 282, 282, 281, 286, 287], [291, 291, 294, 292, 290, 290, 292, 294, 297, 294, 294, 296, 294, 292, 290, 294, 293, 291, 291, 291], [307, 300, 308, 301, 302, 306, 304, 303, 306, 301, 300, 302, 303, 303, 304, 305, 304, 308, 304, 300], [311, 317, 312, 316, 318, 318, 316, 315, 316, 310, 313, 313, 315, 315, 311, 310, 313, 318, 314, 312], [325, 321, 323, 321, 324, 326, 320, 320, 327, 323, 325, 322, 324, 325, 321, 327, 325, 323, 320, 320], [335, 330, 338, 330, 332, 337, 330, 336, 331, 336, 337, 331, 332, 335, 337, 331, 332, 332, 334, 331], [342, 346, 344, 341, 346, 343, 345, 343, 346, 342, 345, 342, 340, 347, 344, 343, 340, 343, 343, 344], [350, 352, 351, 358, 355, 356, 355, 357, 352, 355, 358, 354, 355, 355, 354, 354, 354, 353, 356, 351], [363, 363, 367, 360, 366, 363, 368, 366, 361, 364, 360, 364, 367, 366, 365, 363, 365, 363, 363, 362], [372, 378, 377, 376, 374, 378, 375, 373, 378, 371, 371, 372, 372, 378, 373, 377, 377, 372, 373, 375], [385, 382, 383, 381, 385, 383, 384, 380, 385, 381, 381, 386, 382, 383, 386, 384, 387, 388, 385, 388], [395, 390, 398, 397, 396, 393, 390, 393, 392, 398, 392, 393, 396, 393, 395, 392, 394, 393, 390, 396], [407, 407, 404, 405, 400, 405, 408, 408, 405, 407, 405, 408, 405, 402, 406, 400, 403, 407, 402, 407], [412, 414, 410, 414, 418, 412, 411, 418, 412, 411, 411, 414, 414, 411, 415, 411, 413, 410, 414, 412], [425, 428, 421, 428, 428, 425, 425, 428, 423, 425, 423, 424, 428, 425, 424, 423, 427, 427, 423, 423], [435, 432, 437, 437, 433, 437, 432, 436, 430, 433, 431, 433, 433, 434, 438, 437, 430, 435, 438, 435], [445, 442, 446, 441, 442, 440, 442, 448, 447, 442, 447, 442, 443, 443, 444, 443, 447, 446, 447, 442], [457, 456, 450, 451, 455, 456, 453, 454, 452, 451, 453, 450, 450, 453, 454, 451, 451, 457, 450, 452], [463, 465, 465, 464, 461, 467, 464, 467, 466, 466, 467, 464, 467, 461, 463, 460, 460, 463, 463, 467], [478, 470, 475, 475, 475, 477, 477, 471, 471, 478, 471, 475, 470, 478, 475, 477, 472, 471, 477, 471], [480, 487, 480, 481, 484, 483, 488, 482, 480, 487, 488, 488, 486, 481, 486, 480, 488, 487, 485, 481], [491, 495, 496, 492, 497, 491, 496, 498, 493, 495, 491, 492, 492, 490, 491, 495, 494, 494, 491, 494]]\n",
            "\n",
            "\n",
            "50\n",
            "\n",
            "\n",
            "[[1, 8, 4, 1, 7, 2, 7, 3, 8, 5, 85, 251, 54, 361, 237, 332, 186, 128, 137, 488], [12, 13, 10, 10, 12, 18, 17, 16, 14, 18, 184, 401, 327, 377, 21, 325, 368, 267, 33, 126], [23, 22, 23, 24, 20, 26, 22, 22, 25, 25, 88, 129, 187, 237, 155, 38, 3, 204, 67, 2], [31, 36, 33, 35, 31, 33, 36, 32, 30, 37, 374, 332, 365, 306, 212, 86, 452, 308, 381, 163], [45, 48, 46, 42, 43, 45, 44, 40, 44, 43, 286, 376, 283, 231, 230, 252, 242, 349, 116, 484], [52, 55, 55, 52, 50, 51, 52, 52, 54, 51, 260, 327, 405, 119, 160, 462, 408, 132, 115, 319], [63, 62, 62, 62, 60, 64, 62, 65, 68, 66, 255, 243, 74, 307, 312, 96, 212, 248, 125, 59], [73, 75, 78, 74, 77, 77, 75, 73, 74, 74, 436, 383, 306, 241, 440, 180, 384, 154, 413, 95], [85, 83, 81, 80, 81, 88, 86, 87, 82, 83, 40, 288, 20, 7, 394, 39, 14, 324, 329, 192], [92, 98, 96, 98, 91, 92, 93, 97, 93, 93, 376, 0, 423, 33, 230, 69, 483, 416, 345, 231], [104, 102, 106, 100, 104, 108, 104, 107, 105, 108, 455, 356, 196, 51, 232, 210, 406, 66, 74, 172], [114, 117, 118, 113, 113, 116, 117, 114, 111, 118, 189, 315, 393, 277, 38, 238, 47, 271, 226, 156], [128, 125, 125, 123, 123, 125, 121, 122, 124, 128, 158, 179, 485, 480, 384, 56, 482, 306, 190, 262], [134, 135, 135, 132, 135, 131, 136, 137, 137, 132, 171, 168, 14, 148, 321, 452, 216, 341, 406, 428], [145, 140, 141, 140, 143, 144, 146, 141, 141, 146, 94, 38, 111, 321, 30, 190, 359, 154, 133, 297], [150, 153, 158, 156, 153, 154, 151, 154, 154, 150, 240, 384, 450, 467, 469, 125, 450, 266, 313, 342], [163, 164, 168, 165, 168, 162, 160, 166, 168, 167, 413, 261, 420, 268, 13, 107, 1, 313, 407, 79], [174, 170, 173, 174, 174, 173, 173, 171, 172, 173, 345, 147, 34, 215, 486, 372, 162, 277, 215, 275], [185, 181, 188, 180, 187, 186, 181, 187, 181, 187, 194, 253, 164, 403, 244, 359, 203, 496, 382, 257], [196, 195, 191, 195, 191, 196, 192, 196, 192, 198, 39, 204, 11, 287, 43, 265, 58, 458, 75, 300], [207, 204, 202, 202, 205, 204, 206, 204, 206, 203, 440, 314, 198, 133, 64, 221, 237, 145, 337, 79], [212, 211, 210, 212, 213, 215, 212, 216, 212, 215, 80, 228, 63, 354, 149, 0, 348, 360, 357, 482], [225, 221, 223, 221, 224, 223, 224, 223, 223, 223, 341, 366, 344, 54, 274, 304, 128, 359, 318, 347], [230, 232, 237, 233, 232, 235, 230, 231, 230, 230, 373, 370, 165, 94, 113, 114, 295, 264, 301, 256], [240, 241, 246, 248, 241, 247, 244, 245, 244, 245, 472, 83, 54, 78, 239, 454, 429, 270, 159, 69], [253, 258, 258, 258, 258, 250, 252, 256, 255, 254, 366, 75, 146, 36, 264, 389, 60, 70, 149, 376], [260, 261, 267, 266, 263, 268, 263, 266, 262, 268, 293, 492, 215, 275, 395, 170, 306, 379, 53, 208], [278, 275, 274, 276, 278, 271, 271, 278, 271, 275, 447, 280, 247, 64, 333, 147, 314, 127, 143, 263], [282, 285, 287, 286, 285, 284, 282, 284, 280, 281, 272, 299, 224, 59, 135, 26, 95, 244, 407, 29], [293, 295, 294, 297, 296, 291, 293, 295, 294, 291, 9, 459, 134, 399, 245, 287, 491, 230, 456, 433], [306, 304, 306, 301, 307, 308, 300, 301, 307, 306, 237, 97, 73, 127, 92, 102, 104, 386, 406, 50], [315, 317, 310, 312, 313, 315, 317, 311, 312, 316, 13, 243, 44, 271, 364, 156, 477, 153, 177, 209], [321, 326, 322, 322, 321, 322, 322, 324, 322, 325, 333, 399, 225, 297, 362, 113, 480, 313, 489, 494], [335, 338, 334, 335, 335, 336, 336, 331, 333, 332, 11, 157, 477, 390, 98, 159, 44, 313, 261, 410], [343, 342, 342, 343, 345, 342, 341, 345, 346, 344, 216, 213, 206, 493, 164, 296, 267, 140, 148, 103], [350, 356, 353, 358, 352, 350, 353, 353, 357, 356, 250, 210, 407, 333, 278, 96, 121, 139, 270, 318], [360, 365, 362, 365, 363, 365, 368, 363, 364, 362, 6, 372, 82, 355, 216, 307, 456, 162, 36, 236], [374, 378, 377, 372, 377, 371, 375, 373, 372, 376, 80, 101, 39, 278, 495, 245, 276, 246, 480, 491], [384, 388, 384, 386, 380, 384, 385, 387, 384, 382, 154, 249, 202, 12, 308, 458, 199, 111, 19, 131], [393, 390, 398, 395, 395, 394, 393, 398, 392, 394, 409, 484, 36, 32, 110, 270, 207, 437, 156, 409], [400, 407, 400, 403, 402, 407, 402, 400, 403, 401, 3, 42, 268, 456, 139, 114, 30, 299, 390, 211], [411, 417, 415, 417, 414, 411, 413, 412, 410, 415, 167, 69, 486, 334, 370, 294, 313, 144, 400, 289], [420, 424, 424, 425, 428, 427, 425, 424, 427, 427, 20, 471, 362, 374, 239, 204, 280, 434, 139, 459], [431, 432, 431, 438, 431, 431, 430, 431, 430, 432, 21, 461, 141, 311, 474, 425, 36, 295, 282, 183], [446, 441, 444, 440, 445, 441, 442, 441, 440, 444, 40, 1, 458, 299, 456, 59, 256, 478, 292, 409], [454, 450, 455, 454, 458, 453, 452, 454, 453, 452, 473, 301, 44, 109, 128, 422, 468, 241, 225, 382], [467, 467, 462, 465, 468, 460, 465, 464, 467, 462, 422, 50, 69, 332, 81, 276, 36, 35, 419, 10], [474, 477, 478, 478, 470, 477, 477, 478, 474, 477, 6, 190, 214, 290, 136, 158, 120, 147, 461, 330], [485, 484, 486, 486, 483, 484, 487, 488, 487, 488, 20, 117, 139, 468, 365, 278, 68, 332, 115, 1], [496, 496, 494, 492, 493, 492, 492, 490, 494, 490, 234, 462, 429, 213, 300, 404, 486, 390, 216, 176]]\n",
            "\n",
            "\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lDLjzIZpZ5am",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "3cbc2ed3-1f32-4d19-bea0-f78c053de570"
      },
      "cell_type": "code",
      "source": [
        "#Test dataset\n",
        "#Get positive batches for each speaker with L=10 i.e. 10 positive pairs --> 20 samples\n",
        "#Get negative batches for each speaker with L=10 i.e. 10 negative pairs --> 20 samples\n",
        "\n",
        "positive_indices_test = []\n",
        "negative_indices_test = []\n",
        "for i in range(0,200,10):\n",
        "  positive_indices_test.append(list(np.random.choice(np.arange(i,i+9), 20, replace=True)))\n",
        "  pos = np.arange(i,i+9)\n",
        "  neg = [j for j in range(200) if j not in pos] \n",
        "  negative_indices_test.append(list(np.random.choice(pos, 10, replace=True)) + list(np.random.choice(neg, 10, replace=True)))\n",
        "\n",
        "print (positive_indices_test)\n",
        "print (\"\\n\")\n",
        "print (len(positive_indices_test))\n",
        "print (\"\\n\")\n",
        "print (negative_indices_test)\n",
        "print (\"\\n\")\n",
        "print (len(negative_indices_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8, 8, 4, 8, 1, 6, 3, 0, 4, 7, 2, 8, 7, 0, 0, 1, 6, 5, 7, 1], [11, 14, 12, 18, 14, 13, 14, 16, 17, 15, 16, 14, 14, 11, 13, 12, 13, 18, 10, 15], [28, 25, 23, 24, 28, 21, 20, 24, 27, 28, 21, 20, 25, 25, 25, 26, 20, 23, 20, 21], [32, 37, 37, 37, 32, 38, 36, 32, 35, 30, 33, 30, 35, 33, 37, 30, 30, 33, 37, 36], [42, 42, 41, 45, 46, 43, 48, 41, 45, 40, 44, 46, 47, 45, 47, 43, 41, 45, 44, 46], [52, 58, 55, 50, 54, 51, 52, 57, 53, 55, 56, 54, 57, 51, 58, 53, 54, 58, 54, 52], [63, 60, 60, 68, 60, 68, 60, 61, 63, 65, 62, 66, 60, 63, 62, 60, 62, 60, 64, 61], [75, 76, 74, 78, 72, 75, 73, 70, 76, 72, 76, 75, 70, 72, 74, 70, 71, 76, 73, 72], [83, 80, 82, 80, 84, 85, 80, 87, 81, 87, 88, 85, 85, 84, 88, 80, 82, 87, 87, 81], [93, 95, 94, 93, 93, 92, 96, 94, 97, 96, 96, 94, 93, 93, 91, 92, 97, 98, 92, 97], [103, 108, 106, 102, 108, 105, 108, 107, 103, 102, 105, 105, 106, 108, 101, 102, 102, 107, 105, 106], [110, 116, 111, 110, 116, 118, 115, 111, 114, 116, 118, 115, 118, 116, 113, 115, 118, 117, 113, 118], [122, 125, 126, 124, 121, 128, 126, 124, 121, 128, 122, 125, 126, 124, 124, 124, 127, 120, 122, 126], [131, 135, 130, 137, 138, 131, 138, 138, 132, 138, 131, 134, 135, 131, 137, 136, 138, 137, 135, 134], [145, 146, 141, 146, 140, 141, 145, 146, 147, 140, 147, 141, 147, 140, 144, 140, 142, 141, 147, 146], [152, 151, 153, 155, 156, 155, 153, 154, 150, 150, 150, 152, 151, 158, 156, 155, 157, 150, 158, 151], [163, 165, 160, 164, 164, 160, 162, 163, 165, 165, 164, 167, 164, 166, 162, 164, 163, 166, 167, 168], [176, 171, 173, 171, 174, 178, 176, 176, 175, 170, 177, 172, 177, 174, 172, 176, 174, 172, 175, 176], [183, 188, 184, 183, 186, 183, 180, 183, 182, 182, 186, 182, 181, 182, 181, 186, 188, 184, 180, 180], [198, 191, 192, 192, 195, 191, 196, 198, 196, 190, 191, 193, 195, 195, 196, 198, 194, 196, 192, 191]]\n",
            "\n",
            "\n",
            "20\n",
            "\n",
            "\n",
            "[[8, 0, 2, 4, 5, 5, 7, 1, 3, 8, 173, 127, 68, 78, 49, 122, 144, 146, 136, 186], [16, 15, 16, 17, 13, 10, 13, 17, 17, 11, 179, 102, 146, 86, 198, 159, 64, 118, 93, 88], [22, 25, 22, 25, 28, 21, 27, 26, 20, 20, 54, 114, 30, 110, 90, 130, 134, 123, 91, 54], [30, 32, 36, 32, 33, 34, 32, 33, 35, 30, 86, 197, 27, 27, 53, 160, 64, 186, 147, 3], [44, 41, 45, 47, 45, 48, 48, 45, 48, 40, 98, 141, 141, 105, 169, 161, 32, 127, 97, 195], [58, 52, 53, 51, 58, 57, 54, 55, 51, 58, 179, 19, 109, 137, 137, 81, 176, 97, 124, 124], [67, 65, 68, 64, 67, 64, 60, 67, 62, 60, 33, 13, 5, 140, 129, 157, 1, 171, 28, 51], [73, 73, 77, 75, 76, 75, 71, 77, 76, 72, 107, 100, 123, 135, 149, 153, 125, 165, 17, 20], [87, 81, 84, 87, 87, 85, 83, 88, 82, 83, 17, 115, 160, 5, 140, 91, 119, 114, 186, 100], [92, 90, 90, 98, 91, 96, 96, 90, 98, 90, 63, 66, 55, 37, 86, 190, 18, 153, 141, 45], [106, 104, 103, 103, 108, 103, 108, 103, 102, 108, 74, 183, 160, 44, 60, 10, 193, 60, 171, 111], [118, 110, 116, 114, 111, 112, 117, 117, 117, 117, 6, 124, 41, 152, 159, 85, 153, 138, 16, 196], [120, 126, 122, 126, 128, 128, 120, 123, 128, 125, 143, 166, 75, 65, 194, 129, 16, 181, 110, 102], [132, 138, 137, 138, 131, 137, 135, 136, 130, 132, 110, 67, 65, 102, 19, 48, 122, 177, 180, 187], [148, 146, 141, 143, 144, 148, 146, 140, 140, 147, 17, 30, 99, 46, 73, 35, 68, 159, 83, 121], [157, 157, 155, 157, 153, 157, 157, 153, 151, 157, 24, 41, 58, 192, 186, 194, 44, 5, 23, 35], [160, 168, 163, 164, 166, 164, 168, 166, 161, 167, 79, 179, 1, 54, 85, 79, 170, 114, 195, 122], [173, 178, 174, 173, 170, 176, 172, 173, 176, 170, 40, 198, 62, 150, 45, 118, 90, 141, 74, 184], [183, 188, 188, 185, 183, 184, 186, 184, 182, 188, 122, 168, 153, 105, 51, 67, 60, 162, 157, 74], [198, 191, 198, 190, 197, 191, 197, 194, 192, 193, 149, 100, 182, 144, 175, 12, 29, 127, 46, 53]]\n",
            "\n",
            "\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KUiofwbraO1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create positive and negative batches datasets\n",
        "\n",
        "#Training dataset\n",
        "train_positive_signal_1 = []\n",
        "train_positive_signal_2 = []\n",
        "train_negative_signal_1 = []\n",
        "train_negative_signal_2 = []\n",
        "\n",
        "for i in range(50):\n",
        "  train_positive_signal_1.append([X_abs[j] for j in positive_indices[i][0:10]])\n",
        "  train_positive_signal_2.append([X_abs[j] for j in positive_indices[i][10:20]])\n",
        "  train_negative_signal_1.append([X_abs[j] for j in negative_indices[i][0:10]])\n",
        "  train_negative_signal_2.append([X_abs[j] for j in negative_indices[i][10:20]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OYRQ8Zb6c5PV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "132cdc2f-083d-450f-b9b2-d76cb51b3891"
      },
      "cell_type": "code",
      "source": [
        "len(train_positive_signal_1)\n",
        "len(train_positive_signal_2)\n",
        "len(train_negative_signal_1)\n",
        "len(train_negative_signal_2)\n",
        "\n",
        "len(train_positive_signal_1[0])\n",
        "len(train_positive_signal_2[0])\n",
        "len(train_negative_signal_1[0])\n",
        "len(train_negative_signal_2[0])\n",
        "\n",
        "train_positive_signal_1[0][0].shape\n",
        "train_positive_signal_2[0][0].shape\n",
        "train_negative_signal_1[0][0].shape\n",
        "train_negative_signal_2[0][0].shape\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "ipDf07MGdfA6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test dataset\n",
        "test_positive_signal_1 = []\n",
        "test_positive_signal_2 = []\n",
        "test_negative_signal_1 = []\n",
        "test_negative_signal_2 = []\n",
        "\n",
        "for i in range(20):\n",
        "  test_positive_signal_1.append([X_test_abs[j] for j in positive_indices_test[i][0:10]])\n",
        "  test_positive_signal_2.append([X_test_abs[j] for j in positive_indices_test[i][10:20]])\n",
        "  test_negative_signal_1.append([X_test_abs[j] for j in negative_indices_test[i][0:10]])\n",
        "  test_negative_signal_2.append([X_test_abs[j] for j in negative_indices_test[i][10:20]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GyqFsuKvd7RL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7460dc6e-677c-4f52-d6c7-e87f83ace8ef"
      },
      "cell_type": "code",
      "source": [
        "len(test_positive_signal_1)\n",
        "len(test_positive_signal_2)\n",
        "len(test_negative_signal_1)\n",
        "len(test_negative_signal_2)\n",
        "\n",
        "len(test_positive_signal_1[0])\n",
        "len(test_positive_signal_2[0])\n",
        "len(test_negative_signal_1[0])\n",
        "len(test_negative_signal_2[0])\n",
        "\n",
        "test_positive_signal_1[0][0].shape\n",
        "test_positive_signal_2[0][0].shape\n",
        "test_negative_signal_1[0][0].shape\n",
        "test_negative_signal_2[0][0].shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(513, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "i0NGGCxceLXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Target lists for training and test datasets\n",
        "\n",
        "y_train_positive = [1]*500\n",
        "y_train_negative = [0]*500\n",
        "y_test_positive = [1]*200\n",
        "y_test_negative = [0]*200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZlHd8Pmav9gd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cc479cb-21d3-4db2-b62b-44ca11032286"
      },
      "cell_type": "code",
      "source": [
        "#Stack datasets to create one for training and test\n",
        "\n",
        "train_signal_1 = []\n",
        "train_signal_2 = []\n",
        "train_y = []\n",
        "\n",
        "for i in range(0,50):\n",
        "  train_signal_1.append(np.concatenate((np.array(train_positive_signal_1[i]), np.array(train_negative_signal_1[i]))))\n",
        "  train_signal_2.append(np.concatenate((np.array(train_positive_signal_2[i]), np.array(train_negative_signal_2[i]))))\n",
        "  train_y.append(np.concatenate((np.array(y_train_positive[i*10 : ((i*10)+10)]), np.array(y_train_negative[i*10 : ((i*10)+10)]))))\n",
        "\n",
        "train_signal_1 = list(np.vstack(np.array(train_signal_1)))\n",
        "train_signal_2 = list(np.vstack(np.array(train_signal_2)))\n",
        "train_y = np.ndarray.flatten(np.array(train_y))\n",
        "\n",
        "len(train_signal_1)\n",
        "len(train_signal_2)\n",
        "train_y.shape\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "bQ55IrNH5EfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e851c7ef-4417-46e1-d0ac-3895117db69b"
      },
      "cell_type": "code",
      "source": [
        "test_signal_1 = []\n",
        "test_signal_2 = []\n",
        "test_y = []\n",
        "\n",
        "for i in range(0,20):\n",
        "  test_signal_1.append(np.concatenate((np.array(test_positive_signal_1[i]), np.array(test_negative_signal_1[i]))))\n",
        "  test_signal_2.append(np.concatenate((np.array(test_positive_signal_2[i]), np.array(test_negative_signal_2[i]))))\n",
        "  test_y.append(np.concatenate((np.array(y_test_positive[i*10 : ((i*10)+10)]), np.array(y_test_negative[i*10 : ((i*10)+10)]))))\n",
        "\n",
        "test_signal_1 = list(np.vstack(np.array(test_signal_1)))\n",
        "test_signal_2 = list(np.vstack(np.array(test_signal_2)))\n",
        "test_y = np.ndarray.flatten(np.array(test_y))\n",
        "\n",
        "len(test_signal_1)\n",
        "len(test_signal_2)\n",
        "test_y.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "t8YeSQ8Teyh4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Designing the Siamese Network\n",
        "\n",
        "seq = tf.placeholder(tf.int32, None)\n",
        "dropout_var = tf.placeholder(tf.float32, ())\n",
        "\n",
        "X1 = tf.placeholder(tf.float32, [None, max_len, 513])\n",
        "X2 = tf.placeholder(tf.float32, [None, max_len, 513])\n",
        "y = tf.placeholder(tf.float32, [None])\n",
        "\n",
        "def rnn_function(X, dp):\n",
        "  with tf.variable_scope(\"rnn1\"):\n",
        "    \n",
        "    stacked_rnn = []\n",
        "    for _ in range(5):\n",
        "      r_cell = tf.nn.rnn_cell.BasicLSTMCell(1024, forget_bias=1.0, state_is_tuple=True)\n",
        "      lstm_r_cell = tf.contrib.rnn.DropoutWrapper(r_cell,output_keep_prob=dp)\n",
        "      stacked_rnn.append(lstm_r_cell)\n",
        "    lstm_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn, state_is_tuple=True)\n",
        "\n",
        "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell_m, X, dtype=tf.float32, sequence_length=seq)\n",
        "    dense_output = tf.layers.dense(outputs, 513, kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
        "    d = seq[0]\n",
        "    return [dense_output, d]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCdr4Kzfihl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_function(y,d,batch_size):\n",
        "  tmp= y *tf.square(d)\n",
        "  tmp2 = (1-y) *tf.square(tf.maximum((1 - d),0))\n",
        "  return tf.reduce_sum(tmp +tmp2)/batch_size/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZfMZHG-Jl-Zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "a82c55a5-0b52-4e25-cd92-7dd544650c3d"
      },
      "cell_type": "code",
      "source": [
        "#Output, loss and optimization\n",
        "\n",
        "with tf.variable_scope(\"model\"):\n",
        "  out1 = rnn_function(X1, dropout_var)\n",
        "  output1 = out1[0]\n",
        "  d = out1[1]\n",
        "with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
        "  out2 = rnn_function(X2, dropout_var)\n",
        "  output2 = out2[0]\n",
        "  distance = tf.reduce_sum(tf.diag_part(tf.tensordot( output1[:, :d,:], output2[:, :d,:], axes=[[1],[1]])),1, keep_dims=True) \n",
        "  y_preds = tf.sigmoid(distance)\n",
        "  loss = loss_function(y, y_preds, 20)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate= 0.0000001).minimize(loss)  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-0758bd7eda6a>:14: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-17-0758bd7eda6a>:17: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-17-0758bd7eda6a>:19: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-17-0758bd7eda6a>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-19-2fadfced4f0d>:9: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KJE7G_2YojDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "f7c73fee-ccc9-40b8-d881-5a3859146506"
      },
      "cell_type": "code",
      "source": [
        "#Running model and testing on training and test datasets\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "max_epochs = 40\n",
        "batch_step = 20 #1000 for 50 user batches = 20 batch step i.e. 2L\n",
        "step = 5\n",
        "train_len = np.array([32]*1000)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    avg_cost = 0.\n",
        "    avg_acc = 0.\n",
        "    train_acc = []\n",
        "    random = np.arange(0, 1000, 20)\n",
        "\n",
        "    for i in range(len(random)):\n",
        "        start = int(random[i])\n",
        "        end = int(start + batch_step)\n",
        "        b_x1, b_x2, b_y = np.array(train_signal_1[start:end]).swapaxes(1,2), np.array(train_signal_2[start:end]).swapaxes(1,2), train_y[start:end]\n",
        "        seqlen = np.array(train_len[start:end])\n",
        "        data = {X1: b_x1, X2: b_x2, y: b_y, seq : seqlen, dropout_var : 0.95}\n",
        "        sess.run(optimizer, feed_dict=data)\n",
        "        avg_cost += sess.run(loss, feed_dict=data)\n",
        "        train_preds = sess.run(y_preds, feed_dict=data)\n",
        "        y_pred_binary = np.array([1 if i >= 0.55 else 0 for i in train_preds])\n",
        "        train_acc.append(accuracy_score(b_y, y_pred_binary))\n",
        "     \n",
        "    avg_cost = avg_cost / len(random)\n",
        "    avg_acc = np.mean(train_acc)\n",
        "        \n",
        "    if (epoch+1) % step == 0:\n",
        "        print (\"Epoch: %03d/%03d accuracy: %.9f\" % (epoch, max_epochs, avg_acc))\n",
        "        print (\"Loss\", avg_cost)\n",
        "        \n",
        "print (\"=========================Model Optimization Complete============================\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 004/040 accuracy: 0.524000000\n",
            "Loss 2.8732653522491454\n",
            "Epoch: 009/040 accuracy: 0.555000000\n",
            "Loss 2.6899362134933473\n",
            "Epoch: 014/040 accuracy: 0.587000000\n",
            "Loss 2.624114851951599\n",
            "Epoch: 019/040 accuracy: 0.612000000\n",
            "Loss 2.5922104835510256\n",
            "Epoch: 024/040 accuracy: 0.645000000\n",
            "Loss 2.574125738143921\n",
            "Epoch: 029/040 accuracy: 0.644000000\n",
            "Loss 2.5618166017532347\n",
            "Epoch: 034/040 accuracy: 0.647000000\n",
            "Loss 2.5531778573989867\n",
            "Epoch: 039/040 accuracy: 0.652000000\n",
            "Loss 2.5469803047180175\n",
            "=========================Model Optimization Complete============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5W7Gv0RBqVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fe6b3bb-7ff2-49da-ab47-2c170d763011"
      },
      "cell_type": "code",
      "source": [
        "#Accuracy for test dataset\n",
        "\n",
        "batch_step = 20 #400 for 20 user batches = 20 batch step i.e. 2L\n",
        "step = 5\n",
        "test_len = np.array([45]*400)\n",
        "\n",
        "avg_cost_test = 0.\n",
        "avg_acc_test = 0.\n",
        "test_acc = []\n",
        "random_test = np.arange(0, 400, 20)\n",
        "\n",
        "for i in range(len(random_test)):\n",
        "  start_test = int(random_test[i])\n",
        "  end_test = int(start_test + batch_step)\n",
        "  b_test_x1, b_test_x2, b_test_y = np.array(test_signal_1[start_test:end_test]).swapaxes(1,2), np.array(test_signal_2[start_test:end_test]).swapaxes(1,2), test_y[start_test:end_test]\n",
        "  seqlen_test = np.array(test_len[start_test:end_test])\n",
        "  data_test = {X1: b_test_x1, X2: b_test_x2, y: b_test_y, seq : seqlen_test, dropout_var : 1.0}\n",
        "  avg_cost_test += sess.run(loss, feed_dict=data_test)\n",
        "  test_preds = sess.run(y_preds, feed_dict=data_test)\n",
        "  y_pred_test_binary = np.array([1 if i >= 0.55 else 0 for i in test_preds])\n",
        "  test_acc.append(accuracy_score(b_test_y, y_pred_test_binary))\n",
        "\n",
        "avg_cost_test = avg_cost_test / len(random_test)\n",
        "avg_acc_test = np.mean(test_acc)\n",
        "        \n",
        "print (\"Accuracy:\" , avg_acc_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6249999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}